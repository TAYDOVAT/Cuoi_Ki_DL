{
    "cells":  [
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "bc0f10fa",
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "# !rm -rf /kaggle/working/*\n",
                                     "# %cd /kaggle/working\n",
                                     "# !git clone https://github.com/TAYDOVAT/Cuoi_Ki_DL.git\n",
                                     "# !pip install lpips\n",
                                     "# %cd /kaggle/working/Cuoi_Ki_DL\n",
                                     "\n",
                                     "# !rm -r /kaggle/working/Cuoi_Ki_DL/test\n",
                                     "# !rm -r /kaggle/working/Cuoi_Ki_DL/train\n",
                                     "# !rm -r /kaggle/working/Cuoi_Ki_DL/val\n",
                                     "\n",
                                     "# !cp -r \"/kaggle/input/anh-ve-tinh/Ảnh vệ tinh/test\" /kaggle/working/Cuoi_Ki_DL\n",
                                     "# !cp -r \"/kaggle/input/anh-ve-tinh/Ảnh vệ tinh/train\" /kaggle/working/Cuoi_Ki_DL\n",
                                     "# !cp -r \"/kaggle/input/anh-ve-tinh/Ảnh vệ tinh/val\" /kaggle/working/Cuoi_Ki_DL"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "id":  "3970c4e4",
                      "metadata":  {

                                   },
                      "source":  [
                                     "# Train SRGAN x4"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "ecaef4f2",
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "import os\n",
                                     "import csv\n",
                                     "import random\n",
                                     "import torch\n",
                                     "import torch.distributed as dist\n",
                                     "from ddp_utils import init_distributed, get_device, maybe_wrap_ddp, is_main_process, cleanup_distributed\n",
                                     "from pathlib import Path\n",
                                     "from torch import optim\n",
                                     "from torch.optim import lr_scheduler\n",
                                     "from tqdm.auto import tqdm\n",
                                     "from IPython.display import clear_output\n",
                                     "\n",
                                     "from data import build_loader\n",
                                     "from original_model import SRResNet, DiscriminatorForVGG\n",
                                     "from losses import PixelLoss, PerceptualLoss, AdversarialLoss\n",
                                     "from engine import (\n",
                                     "    train_gan_epoch, val_gan_epoch,\n",
                                     "    save_gan_checkpoint, load_gan_checkpoint,\n",
                                     "    load_gan_history_from_log, rewrite_log_up_to_epoch\n",
                                     ")\n",
                                     "from vis import show_lr_sr_hr, plot_curves\n",
                                     "import lpips"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "9dd709ed",
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "# Config override here\n",
                                     "cfg = {\n",
                                     "    \u0027scale\u0027: 4,\n",
                                     "    \u0027hr_crop\u0027: 96,\n",
                                     "    \u0027gan\u0027: {\n",
                                     "        \u0027batch_size\u0027: 32,\n",
                                     "        \u0027num_workers\u0027: 4,\n",
                                     "        \u0027epochs\u0027: 300,\n",
                                     "        \u0027lr_g\u0027: 1e-5,           # Learning rate cho Generator\n",
                                     "        \u0027lr_d\u0027: 1e-5,           # Learning rate cho Discriminator\n",
                                     "        \u0027adv_weight\u0027: 5e-3,     # Adversarial loss weight\n",
                                     "        \u0027perc_weight\u0027: 1,       # Perceptual loss weight  \n",
                                     "        \u0027pixel_weight\u0027: 0,   # Pixel loss weight\n",
                                     "        \u0027r1_weight\u0027: 10.0,      # R1 gradient penalty\n",
                                     "        \u0027d_steps\u0027: 1,           # Số bước train D mỗi iteration\n",
                                     "        \u0027g_steps\u0027: 2,           # Số bước train G mỗi iteration\n",
                                     "        # ========== RESUME CONFIG ==========\n",
                                     "        \u0027resume\u0027: True,        # True: resume training, False: train từ đầu\n",
                                     "        \u0027load_disc\u0027: True,      # True: load cả Discriminator, False: chỉ load Generator\n",
                                     "        \u0027checkpoint_path\u0027: \u0027weights/gan_checkpoint.pth\u0027,  # Path to checkpoint\n",
                                     "    },\n",
                                     "    \u0027paths\u0027: {\n",
                                     "        \u0027train_lr\u0027: \u0027train/train_lr\u0027,\n",
                                     "        \u0027train_hr\u0027: \u0027train/train_hr\u0027,\n",
                                     "        \u0027val_lr\u0027: \u0027val/val_lr\u0027,\n",
                                     "        \u0027val_hr\u0027: \u0027val/val_hr\u0027,\n",
                                     "        \u0027test_lr\u0027: \u0027test/test_lr\u0027,\n",
                                     "        \u0027test_hr\u0027: \u0027test/test_hr\u0027,\n",
                                     "    },\n",
                                     "}\n",
                                     "base_dir = None\n",
                                     "cwd = Path.cwd().resolve()\n",
                                     "for parent in [cwd] + list(cwd.parents):\n",
                                     "    if (parent / \u0027train\u0027 / \u0027train_lr\u0027).is_dir():\n",
                                     "        base_dir = parent\n",
                                     "        break\n",
                                     "\n",
                                     "if base_dir is None:\n",
                                     "    raise FileNotFoundError(f\"Cannot find \u0027train/train_lr\u0027 from cwd: {cwd}\")\n",
                                     "\n",
                                     "cfg[\u0027paths\u0027][\u0027train_lr\u0027] = str(base_dir / \u0027train\u0027 / \u0027train_lr\u0027)\n",
                                     "cfg[\u0027paths\u0027][\u0027train_hr\u0027] = str(base_dir / \u0027train\u0027 / \u0027train_hr\u0027)\n",
                                     "cfg[\u0027paths\u0027][\u0027val_lr\u0027] = str(base_dir / \u0027val\u0027 / \u0027val_lr\u0027)\n",
                                     "cfg[\u0027paths\u0027][\u0027val_hr\u0027] = str(base_dir / \u0027val\u0027 / \u0027val_hr\u0027)\n",
                                     "cfg[\u0027paths\u0027][\u0027test_lr\u0027] = str(base_dir / \u0027test\u0027 / \u0027test_lr\u0027)\n",
                                     "cfg[\u0027paths\u0027][\u0027test_hr\u0027] = str(base_dir / \u0027test\u0027 / \u0027test_hr\u0027)\n",
                                     "\n",
                                     "is_ddp, local_rank = init_distributed()\n",
                                     "device = get_device(local_rank)\n",
                                     "os.makedirs(\u0027weights\u0027, exist_ok=True)"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "da2aef6b",
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "train_dataset, train_loader = build_loader(\n",
                                     "    cfg[\u0027paths\u0027][\u0027train_lr\u0027], cfg[\u0027paths\u0027][\u0027train_hr\u0027],\n",
                                     "    scale=cfg[\u0027scale\u0027], hr_crop=cfg[\u0027hr_crop\u0027],\n",
                                     "    batch_size=cfg[\u0027gan\u0027][\u0027batch_size\u0027],\n",
                                     "    num_workers=cfg[\u0027gan\u0027][\u0027num_workers\u0027],\n",
                                     "    train=True\n",
                                     ")\n",
                                     "val_dataset, val_loader = build_loader(\n",
                                     "    cfg[\u0027paths\u0027][\u0027val_lr\u0027], cfg[\u0027paths\u0027][\u0027val_hr\u0027],\n",
                                     "    scale=cfg[\u0027scale\u0027], hr_crop=cfg[\u0027hr_crop\u0027],\n",
                                     "    batch_size=8,\n",
                                     "    num_workers=cfg[\u0027gan\u0027][\u0027num_workers\u0027],\n",
                                     "    train=False\n",
                                     ")\n"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "1388d2a7",
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "# ==================== Initialize Models ====================\n",
                                     "generator = SRResNet(upscale=cfg[\u0027scale\u0027]).to(device)\n",
                                     "discriminator = DiscriminatorForVGG().to(device)\n",
                                     "generator = maybe_wrap_ddp(generator, local_rank)\n",
                                     "discriminator = maybe_wrap_ddp(discriminator, local_rank)\n",
                                     "\n",
                                     "# ==================== Initialize Optimizers \u0026 Schedulers ====================\n",
                                     "optimizer_g = optim.Adam(generator.parameters(), lr=cfg[\u0027gan\u0027][\u0027lr_g\u0027])\n",
                                     "optimizer_d = optim.Adam(discriminator.parameters(), lr=cfg[\u0027gan\u0027][\u0027lr_d\u0027])\n",
                                     "scheduler_g = lr_scheduler.StepLR(optimizer_g, step_size=10000, gamma=0.5)\n",
                                     "scheduler_d = lr_scheduler.StepLR(optimizer_d, step_size=10000, gamma=0.5)\n",
                                     "\n",
                                     "# ==================== Loss Criteria ====================\n",
                                     "pixel_criterion = PixelLoss().to(device)\n",
                                     "perceptual_criterion = PerceptualLoss().to(device)\n",
                                     "adversarial_criterion = AdversarialLoss().to(device)\n",
                                     "lpips_metric = lpips.LPIPS(net=\u0027vgg\u0027).to(device)\n",
                                     "\n",
                                     "weights = {\n",
                                     "    \u0027pixel\u0027: cfg[\u0027gan\u0027][\u0027pixel_weight\u0027],\n",
                                     "    \u0027perceptual\u0027: cfg[\u0027gan\u0027][\u0027perc_weight\u0027],\n",
                                     "    \u0027adversarial\u0027: cfg[\u0027gan\u0027][\u0027adv_weight\u0027],\n",
                                     "}\n",
                                     "\n",
                                     "# ==================== Resume or Fresh Start ====================\n",
                                     "log_path = os.path.join(\u0027logs\u0027, \u0027gan_log.csv\u0027)\n",
                                     "\n",
                                     "if cfg[\u0027gan\u0027][\u0027resume\u0027]:\n",
                                     "    # RESUME: Load checkpoint với tất cả states\n",
                                     "    start_epoch, best_lpips = load_gan_checkpoint(\n",
                                     "        generator=generator,\n",
                                     "        discriminator=discriminator,\n",
                                     "        optimizer_g=optimizer_g,\n",
                                     "        optimizer_d=optimizer_d,\n",
                                     "        scheduler_g=scheduler_g,\n",
                                     "        scheduler_d=scheduler_d,\n",
                                     "        path=cfg[\u0027gan\u0027][\u0027checkpoint_path\u0027],\n",
                                     "        load_disc=cfg[\u0027gan\u0027][\u0027load_disc\u0027],\n",
                                     "        device=device\n",
                                     "    )\n",
                                     "    # Load history từ log file\n",
                                     "    history = load_gan_history_from_log(log_path, start_epoch)\n",
                                     "    # Rewrite log file để clean (tránh duplicate)\n",
                                     "    rewrite_log_up_to_epoch(log_path, history, start_epoch)\n",
                                     "else:\n",
                                     "    # FRESH START: Load pretrained SRResNet\n",
                                     "    start_epoch = 1\n",
                                     "    best_lpips = 100.0\n",
                                     "    generator.load_state_dict(torch.load(\u0027weights/best_srresnet.pth\u0027, map_location=device))\n",
                                     "    print(\"[INFO] Loaded Generator from \u0027weights/best_srresnet.pth\u0027\")\n",
                                     "    print(\"[INFO] Initialized fresh Discriminator\")\n",
                                     "    \n",
                                     "    # Empty history\n",
                                     "    history = {\n",
                                     "        \u0027loss_g\u0027: {\u0027train\u0027: [], \u0027val\u0027: []},\n",
                                     "        \u0027loss_d\u0027: {\u0027train\u0027: [], \u0027val\u0027: []},\n",
                                     "        \u0027psnr\u0027: {\u0027train\u0027: [], \u0027val\u0027: []},\n",
                                     "        \u0027ssim\u0027: {\u0027train\u0027: [], \u0027val\u0027: []},\n",
                                     "        \u0027lpips\u0027: {\u0027train\u0027: [], \u0027val\u0027: []},\n",
                                     "        \u0027d_real_prob\u0027: {\u0027train\u0027: [], \u0027val\u0027: []},\n",
                                     "        \u0027d_fake_prob\u0027: {\u0027train\u0027: [], \u0027val\u0027: []},\n",
                                     "    }\n",
                                     "    # Write fresh log header\n",
                                     "    os.makedirs(\u0027logs\u0027, exist_ok=True)\n",
                                     "    with open(log_path, \u0027w\u0027, newline=\u0027\u0027) as f:\n",
                                     "        writer = csv.writer(f)\n",
                                     "        writer.writerow([\n",
                                     "            \u0027epoch\u0027, \u0027train_loss_g\u0027, \u0027val_loss_g\u0027, \u0027train_loss_d\u0027, \u0027val_loss_d\u0027,\n",
                                     "            \u0027train_d_real_prob\u0027, \u0027val_d_real_prob\u0027, \u0027train_d_fake_prob\u0027, \u0027val_d_fake_prob\u0027,\n",
                                     "            \u0027train_psnr\u0027, \u0027val_psnr\u0027, \u0027train_ssim\u0027, \u0027val_ssim\u0027, \u0027train_lpips\u0027, \u0027val_lpips\u0027,\n",
                                     "        ])\n",
                                     "\n",
                                     "print(f\"\\n{\u0027=\u0027*50}\")\n",
                                     "print(f\"Starting from epoch {start_epoch}, best LPIPS: {best_lpips:.4f}\")\n",
                                     "print(f\"Resume: {cfg[\u0027gan\u0027][\u0027resume\u0027]}, Load Disc: {cfg[\u0027gan\u0027][\u0027load_disc\u0027]}\")\n",
                                     "print(f\"{\u0027=\u0027*50}\")"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "0b17e344",
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "epochs = cfg[\u0027gan\u0027][\u0027epochs\u0027]\n",
                                     "\n",
                                     "for epoch in range(start_epoch, epochs + 1):\n",
                                     "    if hasattr(train_loader, \u0027sampler\u0027) and hasattr(train_loader.sampler, \u0027set_epoch\u0027):\n",
                                     "        train_loader.sampler.set_epoch(epoch)\n",
                                     "\n",
                                     "    # ==================== Training ====================\n",
                                     "    train_pbar = tqdm(train_loader, desc=f\u0027Epoch {epoch}/{epochs} [Train]\u0027) if is_main_process() else train_loader\n",
                                     "    train_stats = train_gan_epoch(\n",
                                     "        generator, discriminator, train_pbar,\n",
                                     "        optimizer_g, optimizer_d, device,\n",
                                     "        pixel_criterion, perceptual_criterion, adversarial_criterion,\n",
                                     "        weights,\n",
                                     "        lpips_metric=lpips_metric,\n",
                                     "        g_steps=cfg[\u0027gan\u0027].get(\u0027g_steps\u0027, 1),\n",
                                     "        d_steps=cfg[\u0027gan\u0027].get(\u0027d_steps\u0027, 1),\n",
                                     "        r1_weight=cfg[\u0027gan\u0027].get(\u0027r1_weight\u0027, 0.0),\n",
                                     "    )\n",
                                     "\n",
                                     "    # ==================== Validation ====================\n",
                                     "    val_pbar = tqdm(val_loader, desc=f\u0027Epoch {epoch}/{epochs} [Val]\u0027) if is_main_process() else val_loader\n",
                                     "    val_stats = val_gan_epoch(\n",
                                     "        generator, discriminator, val_pbar, device,\n",
                                     "        pixel_criterion, perceptual_criterion, adversarial_criterion,\n",
                                     "        weights,\n",
                                     "        lpips_metric=lpips_metric\n",
                                     "    )\n",
                                     "\n",
                                     "    # ==================== Step Schedulers ====================\n",
                                     "    scheduler_g.step()\n",
                                     "    scheduler_d.step()\n",
                                     "\n",
                                     "    if is_main_process():\n",
                                     "        # ==================== Update History ====================\n",
                                     "        history[\u0027loss_g\u0027][\u0027train\u0027].append(train_stats[\u0027loss_g\u0027])\n",
                                     "        history[\u0027loss_g\u0027][\u0027val\u0027].append(val_stats[\u0027loss_g\u0027])\n",
                                     "        history[\u0027loss_d\u0027][\u0027train\u0027].append(train_stats[\u0027loss_d\u0027])\n",
                                     "        history[\u0027loss_d\u0027][\u0027val\u0027].append(val_stats[\u0027loss_d\u0027])\n",
                                     "        history[\u0027d_real_prob\u0027][\u0027train\u0027].append(train_stats[\u0027d_real_prob\u0027])\n",
                                     "        history[\u0027d_real_prob\u0027][\u0027val\u0027].append(val_stats[\u0027d_real_prob\u0027])\n",
                                     "        history[\u0027d_fake_prob\u0027][\u0027train\u0027].append(train_stats[\u0027d_fake_prob\u0027])\n",
                                     "        history[\u0027d_fake_prob\u0027][\u0027val\u0027].append(val_stats[\u0027d_fake_prob\u0027])\n",
                                     "        history[\u0027psnr\u0027][\u0027train\u0027].append(train_stats[\u0027psnr\u0027])\n",
                                     "        history[\u0027psnr\u0027][\u0027val\u0027].append(val_stats[\u0027psnr\u0027])\n",
                                     "        history[\u0027ssim\u0027][\u0027train\u0027].append(train_stats[\u0027ssim\u0027])\n",
                                     "        history[\u0027ssim\u0027][\u0027val\u0027].append(val_stats[\u0027ssim\u0027])\n",
                                     "        history[\u0027lpips\u0027][\u0027train\u0027].append(train_stats[\u0027lpips\u0027])\n",
                                     "        history[\u0027lpips\u0027][\u0027val\u0027].append(val_stats[\u0027lpips\u0027])\n",
                                     "\n",
                                     "        # ==================== Append to Log ====================\n",
                                     "        with open(log_path, \u0027a\u0027, newline=\u0027\u0027) as f:\n",
                                     "            writer = csv.writer(f)\n",
                                     "            writer.writerow([\n",
                                     "                epoch,\n",
                                     "                train_stats[\u0027loss_g\u0027],\n",
                                     "                val_stats[\u0027loss_g\u0027],\n",
                                     "                train_stats[\u0027loss_d\u0027],\n",
                                     "                val_stats[\u0027loss_d\u0027],\n",
                                     "                train_stats[\u0027d_real_prob\u0027],\n",
                                     "                val_stats[\u0027d_real_prob\u0027],\n",
                                     "                train_stats[\u0027d_fake_prob\u0027],\n",
                                     "                val_stats[\u0027d_fake_prob\u0027],\n",
                                     "                train_stats[\u0027psnr\u0027],\n",
                                     "                val_stats[\u0027psnr\u0027],\n",
                                     "                train_stats[\u0027ssim\u0027],\n",
                                     "                val_stats[\u0027ssim\u0027],\n",
                                     "                train_stats[\u0027lpips\u0027],\n",
                                     "                val_stats[\u0027lpips\u0027],\n",
                                     "            ])\n",
                                     "\n",
                                     "        # ==================== Save Checkpoint (every epoch) ====================\n",
                                     "        save_gan_checkpoint(\n",
                                     "            generator=generator,\n",
                                     "            discriminator=discriminator,\n",
                                     "            optimizer_g=optimizer_g,\n",
                                     "            optimizer_d=optimizer_d,\n",
                                     "            scheduler_g=scheduler_g,\n",
                                     "            scheduler_d=scheduler_d,\n",
                                     "            epoch=epoch,\n",
                                     "            best_lpips=best_lpips,\n",
                                     "            path=cfg[\u0027gan\u0027][\u0027checkpoint_path\u0027]\n",
                                     "        )\n",
                                     "\n",
                                     "        # Save individual weights (for compatibility)\n",
                                     "        gen_to_save = generator.module if hasattr(generator, \u0027module\u0027) else generator\n",
                                     "        disc_to_save = discriminator.module if hasattr(discriminator, \u0027module\u0027) else discriminator\n",
                                     "        torch.save(gen_to_save.state_dict(), \u0027weights/last_gan.pth\u0027)\n",
                                     "        torch.save(disc_to_save.state_dict(), \u0027weights/last_disc.pth\u0027)\n",
                                     "\n",
                                     "        # ==================== Save Best Model ====================\n",
                                     "        if val_stats[\u0027lpips\u0027] \u003c best_lpips:\n",
                                     "            best_lpips = val_stats[\u0027lpips\u0027]\n",
                                     "            torch.save(gen_to_save.state_dict(), \u0027weights/best_gan.pth\u0027)\n",
                                     "            torch.save(disc_to_save.state_dict(), \u0027weights/best_disc.pth\u0027)\n",
                                     "            print(f\"[NEW BEST] LPIPS: {best_lpips:.4f}\")\n",
                                     "\n",
                                     "        # ==================== Visualization ====================\n",
                                     "        clear_output(wait=True)\n",
                                     "\n",
                                     "        rand_idx = random.randint(0, len(val_dataset) - 1)\n",
                                     "        lr_sample, hr_sample = val_dataset[rand_idx]\n",
                                     "        lr_in = lr_sample.unsqueeze(0).to(device)\n",
                                     "        with torch.no_grad():\n",
                                     "            sr_sample = generator(lr_in).cpu()\n",
                                     "        show_lr_sr_hr(lr_sample, sr_sample, hr_sample)\n",
                                     "\n",
                                     "        plot_curves(history)\n",
                                     "\n",
                                     "        # Print info\n",
                                     "        print(f\"Epoch {epoch}/{epochs} | LR_G: {scheduler_g.get_last_lr()[0]:.6f} | LR_D: {scheduler_d.get_last_lr()[0]:.6f}\")\n",
                                     "        print(f\"Best LPIPS: {best_lpips:.4f}\")\n",
                                     "\n",
                                     "if is_main_process():\n",
                                     "    print(\"\\n\" + \"=\"*50)\n",
                                     "    print(\"GAN Training Completed!\")\n",
                                     "    print(f\"Best LPIPS: {best_lpips:.4f}\")\n",
                                     "    print(\"=\"*50)\n",
                                     "\n",
                                     "if is_ddp:\n",
                                     "    cleanup_distributed()"
                                 ]
                  }
              ],
    "metadata":  {
                     "kernelspec":  {
                                        "display_name":  "(venv) Python 3.13.0 (3.13.0)",
                                        "language":  "python",
                                        "name":  "python3"
                                    },
                     "language_info":  {
                                           "codemirror_mode":  {
                                                                   "name":  "ipython",
                                                                   "version":  3
                                                               },
                                           "file_extension":  ".py",
                                           "mimetype":  "text/x-python",
                                           "name":  "python",
                                           "nbconvert_exporter":  "python",
                                           "pygments_lexer":  "ipython3",
                                           "version":  "3.13.0"
                                       }
                 },
    "nbformat":  4,
    "nbformat_minor":  5
}
