{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0f10fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf /kaggle/working/*\n",
    "# %cd /kaggle/working\n",
    "# !git clone https://github.com/TAYDOVAT/Cuoi_Ki_DL.git\n",
    "# !pip install lpips\n",
    "# %cd /kaggle/working/Cuoi_Ki_DL\n",
    "\n",
    "# !rm -r /kaggle/working/Cuoi_Ki_DL/test\n",
    "# !rm -r /kaggle/working/Cuoi_Ki_DL/train\n",
    "# !rm -r /kaggle/working/Cuoi_Ki_DL/val\n",
    "\n",
    "# !cp -r \"/kaggle/input/anh-ve-tinh/Ảnh vệ tinh/test\" /kaggle/working/Cuoi_Ki_DL\n",
    "# !cp -r \"/kaggle/input/anh-ve-tinh/Ảnh vệ tinh/train\" /kaggle/working/Cuoi_Ki_DL\n",
    "# !cp -r \"/kaggle/input/anh-ve-tinh/Ảnh vệ tinh/val\" /kaggle/working/Cuoi_Ki_DL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3970c4e4",
   "metadata": {},
   "source": [
    "# Train SRGAN x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaef4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from data import build_loader\n",
    "from original_model import SRResNet, DiscriminatorForVGG\n",
    "from losses import PixelLoss, PerceptualLoss, AdversarialLoss\n",
    "from engine import (\n",
    "    train_gan_epoch, val_gan_epoch,\n",
    "    save_gan_checkpoint, load_gan_checkpoint,\n",
    "    load_gan_history_from_log, rewrite_log_up_to_epoch\n",
    ")\n",
    "from vis import show_lr_sr_hr, plot_curves\n",
    "import lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd709ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config override here\n",
    "cfg = {\n",
    "    'scale': 4,\n",
    "    'hr_crop': 96,\n",
    "    'gan': {\n",
    "        'batch_size': 32,\n",
    "        'num_workers': 4,\n",
    "        'epochs': 300,\n",
    "        'lr_g': 1e-5,           # Learning rate cho Generator\n",
    "        'lr_d': 1e-5,           # Learning rate cho Discriminator\n",
    "        'adv_weight': 5e-3,     # Adversarial loss weight\n",
    "        'perc_weight': 1,       # Perceptual loss weight  \n",
    "        'pixel_weight': 0,   # Pixel loss weight\n",
    "        'r1_weight': 10.0,      # R1 gradient penalty\n",
    "        'd_steps': 1,           # Số bước train D mỗi iteration\n",
    "        'g_steps': 2,           # Số bước train G mỗi iteration\n",
    "        # ========== RESUME CONFIG ==========\n",
    "        'resume': True,        # True: resume training, False: train từ đầu\n",
    "        'load_disc': True,      # True: load cả Discriminator, False: chỉ load Generator\n",
    "        'checkpoint_path': 'weights/gan_checkpoint.pth',  # Path to checkpoint\n",
    "    },\n",
    "    'paths': {\n",
    "        'train_lr': 'train/train_lr',\n",
    "        'train_hr': 'train/train_hr',\n",
    "        'val_lr': 'val/val_lr',\n",
    "        'val_hr': 'val/val_hr',\n",
    "        'test_lr': 'test/test_lr',\n",
    "        'test_hr': 'test/test_hr',\n",
    "    },\n",
    "}\n",
    "base_dir = None\n",
    "cwd = Path.cwd().resolve()\n",
    "for parent in [cwd] + list(cwd.parents):\n",
    "    if (parent / 'train' / 'train_lr').is_dir():\n",
    "        base_dir = parent\n",
    "        break\n",
    "\n",
    "if base_dir is None:\n",
    "    raise FileNotFoundError(f\"Cannot find 'train/train_lr' from cwd: {cwd}\")\n",
    "\n",
    "cfg['paths']['train_lr'] = str(base_dir / 'train' / 'train_lr')\n",
    "cfg['paths']['train_hr'] = str(base_dir / 'train' / 'train_hr')\n",
    "cfg['paths']['val_lr'] = str(base_dir / 'val' / 'val_lr')\n",
    "cfg['paths']['val_hr'] = str(base_dir / 'val' / 'val_hr')\n",
    "cfg['paths']['test_lr'] = str(base_dir / 'test' / 'test_lr')\n",
    "cfg['paths']['test_hr'] = str(base_dir / 'test' / 'test_hr')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "os.makedirs('weights', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2aef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, train_loader = build_loader(\n",
    "    cfg['paths']['train_lr'], cfg['paths']['train_hr'],\n",
    "    scale=cfg['scale'], hr_crop=cfg['hr_crop'],\n",
    "    batch_size=cfg['gan']['batch_size'],\n",
    "    num_workers=cfg['gan']['num_workers'],\n",
    "    train=True\n",
    ")\n",
    "val_dataset, val_loader = build_loader(\n",
    "    cfg['paths']['val_lr'], cfg['paths']['val_hr'],\n",
    "    scale=cfg['scale'], hr_crop=cfg['hr_crop'],\n",
    "    batch_size=8,\n",
    "    num_workers=cfg['gan']['num_workers'],\n",
    "    train=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1388d2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Initialize Models ====================\n",
    "generator = SRResNet(upscale=cfg['scale']).to(device)\n",
    "discriminator = DiscriminatorForVGG().to(device)\n",
    "\n",
    "# ==================== Initialize Optimizers & Schedulers ====================\n",
    "optimizer_g = optim.Adam(generator.parameters(), lr=cfg['gan']['lr_g'])\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr=cfg['gan']['lr_d'])\n",
    "scheduler_g = lr_scheduler.StepLR(optimizer_g, step_size=100000, gamma=0.5)\n",
    "scheduler_d = lr_scheduler.StepLR(optimizer_d, step_size=100000, gamma=0.5)\n",
    "\n",
    "# ==================== Loss Criteria ====================\n",
    "pixel_criterion = PixelLoss().to(device)\n",
    "perceptual_criterion = PerceptualLoss().to(device)\n",
    "adversarial_criterion = AdversarialLoss().to(device)\n",
    "lpips_metric = lpips.LPIPS(net='vgg').to(device)\n",
    "\n",
    "weights = {\n",
    "    'pixel': cfg['gan']['pixel_weight'],\n",
    "    'perceptual': cfg['gan']['perc_weight'],\n",
    "    'adversarial': cfg['gan']['adv_weight'],\n",
    "}\n",
    "\n",
    "# ==================== Resume or Fresh Start ====================\n",
    "log_path = os.path.join('logs', 'gan_log.csv')\n",
    "\n",
    "if cfg['gan']['resume']:\n",
    "    # RESUME: Load checkpoint với tất cả states\n",
    "    start_epoch, best_lpips = load_gan_checkpoint(\n",
    "        generator=generator,\n",
    "        discriminator=discriminator,\n",
    "        optimizer_g=optimizer_g,\n",
    "        optimizer_d=optimizer_d,\n",
    "        scheduler_g=scheduler_g,\n",
    "        scheduler_d=scheduler_d,\n",
    "        path=cfg['gan']['checkpoint_path'],\n",
    "        load_disc=cfg['gan']['load_disc'],\n",
    "        device=device\n",
    "    )\n",
    "    # Load history từ log file\n",
    "    history = load_gan_history_from_log(log_path, start_epoch)\n",
    "    # Rewrite log file để clean (tránh duplicate)\n",
    "    rewrite_log_up_to_epoch(log_path, history, start_epoch)\n",
    "else:\n",
    "    # FRESH START: Load pretrained SRResNet\n",
    "    start_epoch = 1\n",
    "    best_lpips = 100.0\n",
    "    generator.load_state_dict(torch.load('weights/best_srresnet.pth', map_location=device))\n",
    "    print(\"[INFO] Loaded Generator from 'weights/best_srresnet.pth'\")\n",
    "    print(\"[INFO] Initialized fresh Discriminator\")\n",
    "    \n",
    "    # Empty history\n",
    "    history = {\n",
    "        'loss_g': {'train': [], 'val': []},\n",
    "        'loss_d': {'train': [], 'val': []},\n",
    "        'psnr': {'train': [], 'val': []},\n",
    "        'ssim': {'train': [], 'val': []},\n",
    "        'lpips': {'train': [], 'val': []},\n",
    "        'd_real_prob': {'train': [], 'val': []},\n",
    "        'd_fake_prob': {'train': [], 'val': []},\n",
    "    }\n",
    "    # Write fresh log header\n",
    "    os.makedirs('logs', exist_ok=True)\n",
    "    with open(log_path, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            'epoch', 'train_loss_g', 'val_loss_g', 'train_loss_d', 'val_loss_d',\n",
    "            'train_d_real_prob', 'val_d_real_prob', 'train_d_fake_prob', 'val_d_fake_prob',\n",
    "            'train_psnr', 'val_psnr', 'train_ssim', 'val_ssim', 'train_lpips', 'val_lpips',\n",
    "        ])\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Starting from epoch {start_epoch}, best LPIPS: {best_lpips:.4f}\")\n",
    "print(f\"Resume: {cfg['gan']['resume']}, Load Disc: {cfg['gan']['load_disc']}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b17e344",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = cfg['gan']['epochs']\n",
    "\n",
    "for epoch in range(start_epoch, epochs + 1):\n",
    "    # ==================== Training ====================\n",
    "    train_pbar = tqdm(train_loader, desc=f'Epoch {epoch}/{epochs} [Train]')\n",
    "    train_stats = train_gan_epoch(\n",
    "        generator, discriminator, train_pbar,\n",
    "        optimizer_g, optimizer_d, device,\n",
    "        pixel_criterion, perceptual_criterion, adversarial_criterion,\n",
    "        weights,\n",
    "        lpips_metric=lpips_metric,\n",
    "        g_steps=cfg['gan'].get('g_steps', 1),\n",
    "        d_steps=cfg['gan'].get('d_steps', 1),\n",
    "        r1_weight=cfg['gan'].get('r1_weight', 0.0),\n",
    "    )\n",
    "\n",
    "    # ==================== Validation ====================\n",
    "    val_pbar = tqdm(val_loader, desc=f'Epoch {epoch}/{epochs} [Val]')\n",
    "    val_stats = val_gan_epoch(\n",
    "        generator, discriminator, val_pbar, device,\n",
    "        pixel_criterion, perceptual_criterion, adversarial_criterion,\n",
    "        weights,\n",
    "        lpips_metric=lpips_metric\n",
    "    )\n",
    "\n",
    "    # ==================== Step Schedulers ====================\n",
    "    scheduler_g.step()\n",
    "    scheduler_d.step()\n",
    "\n",
    "    # ==================== Update History ====================\n",
    "    history['loss_g']['train'].append(train_stats['loss_g'])\n",
    "    history['loss_g']['val'].append(val_stats['loss_g'])\n",
    "    history['loss_d']['train'].append(train_stats['loss_d'])\n",
    "    history['loss_d']['val'].append(val_stats['loss_d'])\n",
    "    history['d_real_prob']['train'].append(train_stats['d_real_prob'])\n",
    "    history['d_real_prob']['val'].append(val_stats['d_real_prob'])\n",
    "    history['d_fake_prob']['train'].append(train_stats['d_fake_prob'])\n",
    "    history['d_fake_prob']['val'].append(val_stats['d_fake_prob'])\n",
    "    history['psnr']['train'].append(train_stats['psnr'])\n",
    "    history['psnr']['val'].append(val_stats['psnr'])\n",
    "    history['ssim']['train'].append(train_stats['ssim'])\n",
    "    history['ssim']['val'].append(val_stats['ssim'])\n",
    "    history['lpips']['train'].append(train_stats['lpips'])\n",
    "    history['lpips']['val'].append(val_stats['lpips'])\n",
    "\n",
    "    # ==================== Append to Log ====================\n",
    "    with open(log_path, 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\n",
    "            epoch,\n",
    "            train_stats['loss_g'],\n",
    "            val_stats['loss_g'],\n",
    "            train_stats['loss_d'],\n",
    "            val_stats['loss_d'],\n",
    "            train_stats['d_real_prob'],\n",
    "            val_stats['d_real_prob'],\n",
    "            train_stats['d_fake_prob'],\n",
    "            val_stats['d_fake_prob'],\n",
    "            train_stats['psnr'],\n",
    "            val_stats['psnr'],\n",
    "            train_stats['ssim'],\n",
    "            val_stats['ssim'],\n",
    "            train_stats['lpips'],\n",
    "            val_stats['lpips'],\n",
    "        ])\n",
    "\n",
    "    # ==================== Save Checkpoint (every epoch) ====================\n",
    "    epoch_dir = os.path.join('weights', f'epoch_{epoch}')\n",
    "    os.makedirs(epoch_dir, exist_ok=True)\n",
    "\n",
    "    # Rolling checkpoint for resume\n",
    "    save_gan_checkpoint(\n",
    "        generator=generator,\n",
    "        discriminator=discriminator,\n",
    "        optimizer_g=optimizer_g,\n",
    "        optimizer_d=optimizer_d,\n",
    "        scheduler_g=scheduler_g,\n",
    "        scheduler_d=scheduler_d,\n",
    "        epoch=epoch,\n",
    "        best_lpips=best_lpips,\n",
    "        path=cfg['gan']['checkpoint_path']\n",
    "    )\n",
    "\n",
    "    # Epoch snapshot checkpoint\n",
    "    save_gan_checkpoint(\n",
    "        generator=generator,\n",
    "        discriminator=discriminator,\n",
    "        optimizer_g=optimizer_g,\n",
    "        optimizer_d=optimizer_d,\n",
    "        scheduler_g=scheduler_g,\n",
    "        scheduler_d=scheduler_d,\n",
    "        epoch=epoch,\n",
    "        best_lpips=best_lpips,\n",
    "        path=os.path.join(epoch_dir, f'gan_checkpoint_{epoch}.pth')\n",
    "    )\n",
    "    \n",
    "    # Save individual weights (for compatibility)\n",
    "    torch.save(generator.state_dict(), 'weights/last_gan.pth')\n",
    "    torch.save(discriminator.state_dict(), 'weights/last_disc.pth')\n",
    "    \n",
    "    # Save individual weights per epoch\n",
    "    torch.save(generator.state_dict(), os.path.join(epoch_dir, f'generator_{epoch}.pth'))\n",
    "    torch.save(discriminator.state_dict(), os.path.join(epoch_dir, f'discriminator_{epoch}.pth'))\n",
    "\n",
    "# ==================== Save Best Model ====================\n",
    "    if val_stats['lpips'] < best_lpips:\n",
    "        best_lpips = val_stats['lpips']\n",
    "        torch.save(generator.state_dict(), 'weights/best_gan.pth')\n",
    "        torch.save(discriminator.state_dict(), 'weights/best_disc.pth')\n",
    "        print(f\"[NEW BEST] LPIPS: {best_lpips:.4f}\")\n",
    "\n",
    "    # ==================== Visualization ====================\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    rand_idx = random.randint(0, len(val_dataset) - 1)\n",
    "    lr_sample, hr_sample = val_dataset[rand_idx]\n",
    "    lr_in = lr_sample.unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        sr_sample = generator(lr_in).cpu()\n",
    "    show_lr_sr_hr(lr_sample, sr_sample, hr_sample)\n",
    "\n",
    "    plot_curves(history)\n",
    "    \n",
    "    # Print info\n",
    "    print(f\"Epoch {epoch}/{epochs} | LR_G: {scheduler_g.get_last_lr()[0]:.6f} | LR_D: {scheduler_d.get_last_lr()[0]:.6f}\")\n",
    "    print(f\"Best LPIPS: {best_lpips:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GAN Training Completed!\")\n",
    "print(f\"Best LPIPS: {best_lpips:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(venv) Python 3.13.0 (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
