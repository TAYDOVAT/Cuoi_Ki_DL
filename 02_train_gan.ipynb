{
    "cells":  [
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "bc0f10fa",
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "# !rm -rf /kaggle/working/*\n",
                                     "# %cd /kaggle/working\n",
                                     "# !git clone --branch Distributed-Data-Parallel https://github.com/TAYDOVAT/Cuoi_Ki_DL.git\n",
                                     "# !pip install lpips\n",
                                     "# %cd /kaggle/working/Cuoi_Ki_DL\n",
                                     "\n",
                                     "# !rm -r /kaggle/working/Cuoi_Ki_DL/test\n",
                                     "# !rm -r /kaggle/working/Cuoi_Ki_DL/train\n",
                                     "# !rm -r /kaggle/working/Cuoi_Ki_DL/val"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "id":  "3970c4e4",
                      "metadata":  {

                                   },
                      "source":  [
                                     "# Train SRGAN x4"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "ecaef4f2",
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "import os\n",
                                     "import csv\n",
                                     "import random\n",
                                     "import torch\n",
                                     "import torch.distributed as dist\n",
                                     "from ddp_utils import init_distributed, get_device, maybe_wrap_ddp, is_main_process, cleanup_distributed\n",
                                     "from pathlib import Path\n",
                                     "from torch import optim\n",
                                     "from torch.optim import lr_scheduler\n",
                                     "from tqdm.auto import tqdm\n",
                                     "from IPython.display import clear_output\n",
                                     "\n",
                                     "from data import build_loader\n",
                                     "from original_model import SRResNet, DiscriminatorForVGG\n",
                                     "from losses import PixelLoss, PerceptualLoss, AdversarialLoss\n",
                                     "from engine import (\n",
                                     "    train_gan_epoch, val_gan_epoch,\n",
                                     "    save_gan_checkpoint, load_gan_checkpoint,\n",
                                     "    load_gan_history_from_log, rewrite_log_up_to_epoch\n",
                                     ")\n",
                                     "from vis import show_lr_sr_hr, plot_curves\n",
                                     "import lpips"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "9dd709ed",
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "# Config override here\n",
                                     "cfg = {\n",
                                     "    \u0027scale\u0027: 4,\n",
                                     "    \u0027hr_crop\u0027: 96,\n",
                                     "    \u0027gan\u0027: {\n",
                                     "        \u0027batch_size\u0027: 32,\n",
                                     "        \u0027num_workers\u0027: 4,\n",
                                     "        \u0027epochs\u0027: 300,\n",
                                     "        \u0027lr_g\u0027: 1e-5,           # Learning rate cho Generator\n",
                                     "        \u0027lr_d\u0027: 1e-5,           # Learning rate cho Discriminator\n",
                                     "        \u0027adv_weight\u0027: 1e-2,     # Adversarial loss weight\n",
                                     "        \u0027perc_weight\u0027: 1,       # Perceptual loss weight  \n",
                                     "        \u0027pixel_weight\u0027: 0,   # Pixel loss weight\n",
                                     "        \u0027r1_weight\u0027: 10.0,      # R1 gradient penalty\n",
                                     "        \u0027use_amp\u0027: True,        # Automatic Mixed Precision\n",
                                     "        \u0027d_steps\u0027: 1,           # Số bước train D mỗi iteration\n",
                                     "        \u0027g_steps\u0027: 2,           # Số bước train G mỗi iteration\n",
                                     "        # ========== RESUME CONFIG ==========\n",
                                     "        \u0027resume\u0027: False,        # True: resume training, False: train từ đầu\n",
                                     "        \u0027load_disc\u0027: False,      # True: load cả Discriminator, False: chỉ load Generator\n",
                                     "        \u0027checkpoint_path\u0027: \u0027weights/gan_checkpoint.pth\u0027,  # Path to checkpoint\n",
                                     "    },\n",
                                     "    \u0027paths\u0027: {\n",
                                     "        \u0027train_lr\u0027: \u0027train/train_lr\u0027,\n",
                                     "        \u0027train_hr\u0027: \u0027train/train_hr\u0027,\n",
                                     "        \u0027val_lr\u0027: \u0027val/val_lr\u0027,\n",
                                     "        \u0027val_hr\u0027: \u0027val/val_hr\u0027,\n",
                                     "        \u0027test_lr\u0027: \u0027test/test_lr\u0027,\n",
                                     "        \u0027test_hr\u0027: \u0027test/test_hr\u0027,\n",
                                     "    },\n",
                                     "}\n",
                                     "data_root = os.environ.get(\u0027DATA_ROOT\u0027)\n",
                                     "kaggle_root = Path(\u0027/kaggle/input/anh-ve-tinh-2/Anh_ve_tinh_2\u0027)\n",
                                     "base_dir = None\n",
                                     "cwd = Path.cwd().resolve()\n",
                                     "\n",
                                     "candidate = None\n",
                                     "if data_root:\n",
                                     "    candidate = Path(data_root).expanduser().resolve()\n",
                                     "elif kaggle_root.is_dir():\n",
                                     "    candidate = kaggle_root\n",
                                     "\n",
                                     "if candidate is not None and (candidate / \u0027train\u0027 / \u0027train_lr\u0027).is_dir():\n",
                                     "    base_dir = candidate\n",
                                     "else:\n",
                                     "    train_lr_path = Path(cfg[\u0027paths\u0027][\u0027train_lr\u0027])\n",
                                     "    if train_lr_path.is_absolute():\n",
                                     "        if train_lr_path.is_dir():\n",
                                     "            base_dir = train_lr_path.parents[1]\n",
                                     "    else:\n",
                                     "        candidate = (cwd / train_lr_path).resolve()\n",
                                     "        if candidate.is_dir():\n",
                                     "            base_dir = candidate.parents[1]\n",
                                     "\n",
                                     "if base_dir is None:\n",
                                     "    for parent in [cwd] + list(cwd.parents):\n",
                                     "        if (parent / \u0027train\u0027 / \u0027train_lr\u0027).is_dir():\n",
                                     "            base_dir = parent\n",
                                     "            break\n",
                                     "\n",
                                     "if base_dir is None:\n",
                                     "    raise FileNotFoundError(\n",
                                     "        f\"Cannot find dataset root. Set DATA_ROOT or update cfg[\u0027paths\u0027] (cwd: {cwd})\"\n",
                                     "    )\n",
                                     "\n",
                                     "cfg[\u0027paths\u0027][\u0027train_lr\u0027] = str(base_dir / \u0027train\u0027 / \u0027train_lr\u0027)\n",
                                     "cfg[\u0027paths\u0027][\u0027train_hr\u0027] = str(base_dir / \u0027train\u0027 / \u0027train_hr\u0027)\n",
                                     "cfg[\u0027paths\u0027][\u0027val_lr\u0027] = str(base_dir / \u0027val\u0027 / \u0027val_lr\u0027)\n",
                                     "cfg[\u0027paths\u0027][\u0027val_hr\u0027] = str(base_dir / \u0027val\u0027 / \u0027val_hr\u0027)\n",
                                     "cfg[\u0027paths\u0027][\u0027test_lr\u0027] = str(base_dir / \u0027test\u0027 / \u0027test_lr\u0027)\n",
                                     "cfg[\u0027paths\u0027][\u0027test_hr\u0027] = str(base_dir / \u0027test\u0027 / \u0027test_hr\u0027)\n",
                                     "\n",
                                     "USE_DDP_SPAWN = torch.cuda.is_available() and torch.cuda.device_count() \u003e 1\n",
                                     "if USE_DDP_SPAWN:\n",
                                     "    os.environ[\u0027DISABLE_DP\u0027] = \u00271\u0027\n",
                                     "    is_ddp = False\n",
                                     "    local_rank = 0\n",
                                     "    device = torch.device(\u0027cpu\u0027)\n",
                                     "else:\n",
                                     "    is_ddp, local_rank = init_distributed()\n",
                                     "    device = get_device(local_rank)\n",
                                     "    os.makedirs(\u0027weights\u0027, exist_ok=True)"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "da2aef6b",
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "if not USE_DDP_SPAWN:\n",
                                     "    train_dataset, train_loader = build_loader(\n",
                                     "        cfg[\u0027paths\u0027][\u0027train_lr\u0027], cfg[\u0027paths\u0027][\u0027train_hr\u0027],\n",
                                     "        scale=cfg[\u0027scale\u0027], hr_crop=cfg[\u0027hr_crop\u0027],\n",
                                     "        batch_size=cfg[\u0027gan\u0027][\u0027batch_size\u0027],\n",
                                     "        num_workers=cfg[\u0027gan\u0027][\u0027num_workers\u0027],\n",
                                     "        train=True\n",
                                     "    )\n",
                                     "    val_dataset, val_loader = build_loader(\n",
                                     "        cfg[\u0027paths\u0027][\u0027val_lr\u0027], cfg[\u0027paths\u0027][\u0027val_hr\u0027],\n",
                                     "        scale=cfg[\u0027scale\u0027], hr_crop=cfg[\u0027hr_crop\u0027],\n",
                                        "        batch_size=8,\n",
                                     "        num_workers=cfg[\u0027gan\u0027][\u0027num_workers\u0027],\n",
                                     "        train=False\n",
                                     "    )\n"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "1388d2a7",
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "# ==================== Initialize Models ====================\n",
                                     "generator = SRResNet(upscale=cfg[\u0027scale\u0027]).to(device)\n",
                                     "discriminator = DiscriminatorForVGG().to(device)\n",
                                     "generator = maybe_wrap_ddp(generator, local_rank)\n",
                                     "discriminator = maybe_wrap_ddp(discriminator, local_rank)\n",
                                     "\n",
                                     "# ==================== Initialize Optimizers \u0026 Schedulers ====================\n",
                                     "optimizer_g = optim.Adam(generator.parameters(), lr=cfg[\u0027gan\u0027][\u0027lr_g\u0027])\n",
                                     "optimizer_d = optim.Adam(discriminator.parameters(), lr=cfg[\u0027gan\u0027][\u0027lr_d\u0027])\n",
                                     "scheduler_g = lr_scheduler.StepLR(optimizer_g, step_size=10000, gamma=0.5)\n",
                                     "scheduler_d = lr_scheduler.StepLR(optimizer_d, step_size=10000, gamma=0.5)\n",
                                     "\n",
                                     "# ==================== Loss Criteria ====================\n",
                                     "pixel_criterion = PixelLoss().to(device)\n",
                                     "perceptual_criterion = PerceptualLoss().to(device)\n",
                                     "adversarial_criterion = AdversarialLoss().to(device)\n",
                                     "lpips_metric = lpips.LPIPS(net=\u0027vgg\u0027).to(device)\n",
                                     "\n",
                                     "weights = {\n",
                                     "    \u0027pixel\u0027: cfg[\u0027gan\u0027][\u0027pixel_weight\u0027],\n",
                                     "    \u0027perceptual\u0027: cfg[\u0027gan\u0027][\u0027perc_weight\u0027],\n",
                                     "    \u0027adversarial\u0027: cfg[\u0027gan\u0027][\u0027adv_weight\u0027],\n",
                                     "}\n",
                                     "use_amp = cfg[\u0027gan\u0027].get(\u0027use_amp\u0027, False)\n",
                                     "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
                                     "\n",
                                     "# ==================== Resume or Fresh Start ====================\n",
                                     "log_path = os.path.join(\u0027logs\u0027, \u0027gan_log.csv\u0027)\n",
                                     "\n",
                                     "if cfg[\u0027gan\u0027][\u0027resume\u0027]:\n",
                                     "    # RESUME: Load checkpoint với tất cả states\n",
                                     "    start_epoch, best_lpips = load_gan_checkpoint(\n",
                                     "        generator=generator,\n",
                                     "        discriminator=discriminator,\n",
                                     "        optimizer_g=optimizer_g,\n",
                                     "        optimizer_d=optimizer_d,\n",
                                     "        scheduler_g=scheduler_g,\n",
                                     "        scheduler_d=scheduler_d,\n",
                                     "        path=cfg[\u0027gan\u0027][\u0027checkpoint_path\u0027],\n",
                                     "        load_disc=cfg[\u0027gan\u0027][\u0027load_disc\u0027],\n",
                                     "        device=device\n",
                                     "    )\n",
                                     "    # Load history từ log file\n",
                                     "    history = load_gan_history_from_log(log_path, start_epoch)\n",
                                     "    # Rewrite log file để clean (tránh duplicate)\n",
                                     "    rewrite_log_up_to_epoch(log_path, history, start_epoch)\n",
                                     "else:\n",
                                     "    # FRESH START: Load pretrained SRResNet\n",
                                     "    start_epoch = 1\n",
                                     "    best_lpips = 100.0\n",
                                     "    def _load_state_flexible(model, path, device):\n",
                                     "        sd = torch.load(path, map_location=device)\n",
                                     "        if isinstance(sd, dict) and \u0027state_dict\u0027 in sd:\n",
                                     "            sd = sd[\u0027state_dict\u0027]\n",
                                     "        # If checkpoint keys are prefixed with \u0027module.\u0027, strip it\n",
                                     "        if isinstance(sd, dict) and sd and next(iter(sd)).startswith(\u0027module.\u0027):\n",
                                     "            sd = {k[len(\u0027module.\u0027):]: v for k, v in sd.items()}\n",
                                     "        target = model.module if hasattr(model, \u0027module\u0027) else model\n",
                                     "        target.load_state_dict(sd, strict=True)\n",
                                     "\n",
                                     "    _load_state_flexible(generator, \u0027weights/best_srresnet.pth\u0027, device)\n",
                                     "    print(\"[INFO] Loaded Generator from \u0027weights/best_srresnet.pth\u0027\")\n",
                                     "    print(\"[INFO] Initialized fresh Discriminator\")\n",
                                     "    \n",
                                     "    # Empty history\n",
                                     "    history = {\n",
                                     "        \u0027loss_g\u0027: {\u0027train\u0027: [], \u0027val\u0027: []},\n",
                                     "        \u0027loss_d\u0027: {\u0027train\u0027: [], \u0027val\u0027: []},\n",
                                     "        \u0027psnr\u0027: {\u0027train\u0027: [], \u0027val\u0027: []},\n",
                                     "        \u0027ssim\u0027: {\u0027train\u0027: [], \u0027val\u0027: []},\n",
                                     "        \u0027lpips\u0027: {\u0027train\u0027: [], \u0027val\u0027: []},\n",
                                     "        \u0027d_real_prob\u0027: {\u0027train\u0027: [], \u0027val\u0027: []},\n",
                                     "        \u0027d_fake_prob\u0027: {\u0027train\u0027: [], \u0027val\u0027: []},\n",
                                     "    }\n",
                                     "    # Write fresh log header\n",
                                     "    os.makedirs(\u0027logs\u0027, exist_ok=True)\n",
                                     "    with open(log_path, \u0027w\u0027, newline=\u0027\u0027) as f:\n",
                                     "        writer = csv.writer(f)\n",
                                     "        writer.writerow([\n",
                                     "            \u0027epoch\u0027, \u0027train_loss_g\u0027, \u0027val_loss_g\u0027, \u0027train_loss_d\u0027, \u0027val_loss_d\u0027,\n",
                                     "            \u0027train_d_real_prob\u0027, \u0027val_d_real_prob\u0027, \u0027train_d_fake_prob\u0027, \u0027val_d_fake_prob\u0027,\n",
                                     "            \u0027train_psnr\u0027, \u0027val_psnr\u0027, \u0027train_ssim\u0027, \u0027val_ssim\u0027, \u0027train_lpips\u0027, \u0027val_lpips\u0027,\n",
                                     "        ])\n",
                                     "\n",
                                     "print(f\"\\n{\u0027=\u0027*50}\")\n",
                                     "print(f\"Starting from epoch {start_epoch}, best LPIPS: {best_lpips:.4f}\")\n",
                                     "print(f\"Resume: {cfg[\u0027gan\u0027][\u0027resume\u0027]}, Load Disc: {cfg[\u0027gan\u0027][\u0027load_disc\u0027]}\")\n",
                                     "print(f\"{\u0027=\u0027*50}\")"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "0b17e344",
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "if USE_DDP_SPAWN:\n",
                                     "    import torch.multiprocessing as mp\n",
                                     "    def _ddp_worker(rank, world_size, cfg):\n",
                                     "        is_ddp, local_rank = init_distributed(rank=rank, world_size=world_size, init_method=f\"tcp://127.0.0.1:{os.environ.get(\u0027MASTER_PORT\u0027,\u002729500\u0027)}\")\n",
                                     "        device = get_device(local_rank)\n",
                                     "        os.makedirs(\u0027weights\u0027, exist_ok=True)\n",
                                     "        os.makedirs(\u0027logs\u0027, exist_ok=True)\n",
                                     "\n",
                                     "        train_dataset, train_loader = build_loader(\n",
                                     "            cfg[\u0027paths\u0027][\u0027train_lr\u0027], cfg[\u0027paths\u0027][\u0027train_hr\u0027],\n",
                                     "            scale=cfg[\u0027scale\u0027], hr_crop=cfg[\u0027hr_crop\u0027],\n",
                                     "            batch_size=cfg[\u0027gan\u0027][\u0027batch_size\u0027],\n",
                                     "            num_workers=cfg[\u0027gan\u0027][\u0027num_workers\u0027],\n",
                                     "            train=True\n",
                                     "        )\n",
                                     "        val_batch_size = max(8, world_size * 4)\n",
                                     "        val_batch_size = (val_batch_size // world_size) * world_size\n",
                                     "        val_dataset, val_loader = build_loader(\n",
                                     "            cfg[\u0027paths\u0027][\u0027val_lr\u0027], cfg[\u0027paths\u0027][\u0027val_hr\u0027],\n",
                                     "            scale=cfg[\u0027scale\u0027], hr_crop=cfg[\u0027hr_crop\u0027],\n",
                                     "            batch_size=val_batch_size,\n",
                                     "            num_workers=cfg[\u0027gan\u0027][\u0027num_workers\u0027],\n",
                                     "            train=False\n",
                                     "        )\n",
                                     "\n",
                                     "        # Models\n",
                                     "        generator = SRResNet(upscale=cfg[\u0027scale\u0027]).to(device)\n",
                                     "        discriminator = DiscriminatorForVGG().to(device)\n",
                                     "        generator = maybe_wrap_ddp(generator, local_rank)\n",
                                     "        discriminator = maybe_wrap_ddp(discriminator, local_rank)\n",
                                     "\n",
                                     "        # Optimizers \u0026 Schedulers\n",
                                     "        optimizer_g = optim.Adam(generator.parameters(), lr=cfg[\u0027gan\u0027][\u0027lr_g\u0027])\n",
                                     "        optimizer_d = optim.Adam(discriminator.parameters(), lr=cfg[\u0027gan\u0027][\u0027lr_d\u0027])\n",
                                     "        scheduler_g = lr_scheduler.StepLR(optimizer_g, step_size=10000, gamma=0.5)\n",
                                     "        scheduler_d = lr_scheduler.StepLR(optimizer_d, step_size=10000, gamma=0.5)\n",
                                     "\n",
                                     "        # Losses\n",
                                     "        pixel_criterion = PixelLoss().to(device)\n",
                                     "        perceptual_criterion = PerceptualLoss().to(device)\n",
                                     "        adversarial_criterion = AdversarialLoss().to(device)\n",
                                     "        lpips_metric = lpips.LPIPS(net=\u0027vgg\u0027).to(device)\n",
                                     "\n",
                                     "        weights = {\n",
                                     "            \u0027pixel\u0027: cfg[\u0027gan\u0027][\u0027pixel_weight\u0027],\n",
                                     "            \u0027perceptual\u0027: cfg[\u0027gan\u0027][\u0027perc_weight\u0027],\n",
                                     "            \u0027adversarial\u0027: cfg[\u0027gan\u0027][\u0027adv_weight\u0027],\n",
                                     "        }\n",
                                     "        use_amp = cfg[\u0027gan\u0027].get(\u0027use_amp\u0027, False)\n",
                                     "        scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
                                     "\n",
                                     "        def _empty_history():\n",
                                     "            return {\n",
                                     "                \u0027loss_g\u0027: {\u0027train\u0027: [], \u0027val\u0027: []},\n",
                                     "                \u0027loss_d\u0027: {\u0027train\u0027: [], \u0027val\u0027: []},\n",
                                     "                \u0027psnr\u0027: {\u0027train\u0027: [], \u0027val\u0027: []},\n",
                                     "                \u0027ssim\u0027: {\u0027train\u0027: [], \u0027val\u0027: []},\n",
                                     "                \u0027lpips\u0027: {\u0027train\u0027: [], \u0027val\u0027: []},\n",
                                     "                \u0027d_real_prob\u0027: {\u0027train\u0027: [], \u0027val\u0027: []},\n",
                                     "                \u0027d_fake_prob\u0027: {\u0027train\u0027: [], \u0027val\u0027: []},\n",
                                     "            }\n",
                                     "\n",
                                     "        log_path = os.path.join(\u0027logs\u0027, \u0027gan_log.csv\u0027)\n",
                                     "        if cfg[\u0027gan\u0027][\u0027resume\u0027]:\n",
                                     "            start_epoch, best_lpips = load_gan_checkpoint(\n",
                                     "                generator=generator,\n",
                                     "                discriminator=discriminator,\n",
                                     "                optimizer_g=optimizer_g,\n",
                                     "                optimizer_d=optimizer_d,\n",
                                     "                scheduler_g=scheduler_g,\n",
                                     "                scheduler_d=scheduler_d,\n",
                                     "                path=cfg[\u0027gan\u0027][\u0027checkpoint_path\u0027],\n",
                                     "                load_disc=cfg[\u0027gan\u0027][\u0027load_disc\u0027],\n",
                                     "                device=device\n",
                                     "            )\n",
                                     "            if is_main_process():\n",
                                     "                history = load_gan_history_from_log(log_path, start_epoch)\n",
                                     "                rewrite_log_up_to_epoch(log_path, history, start_epoch)\n",
                                     "            else:\n",
                                     "                history = _empty_history()\n",
                                     "        else:\n",
                                     "            start_epoch = 1\n",
                                     "            best_lpips = 100.0\n",
                                     "            def _load_state_flexible(model, path, device):\n",
                                     "                sd = torch.load(path, map_location=device)\n",
                                     "                if isinstance(sd, dict) and \u0027state_dict\u0027 in sd:\n",
                                     "                    sd = sd[\u0027state_dict\u0027]\n",
                                     "                if isinstance(sd, dict) and sd and next(iter(sd)).startswith(\u0027module.\u0027):\n",
                                     "                    sd = {k[len(\u0027module.\u0027):]: v for k, v in sd.items()}\n",
                                     "                target = model.module if hasattr(model, \u0027module\u0027) else model\n",
                                     "                target.load_state_dict(sd, strict=True)\n",
                                     "\n",
                                     "            _load_state_flexible(generator, \u0027weights/best_srresnet.pth\u0027, device)\n",
                                     "            if is_main_process():\n",
                                     "                print(\"[INFO] Loaded Generator from \u0027weights/best_srresnet.pth\u0027\")\n",
                                     "                print(\"[INFO] Initialized fresh Discriminator\")\n",
                                     "            history = _empty_history()\n",
                                     "            if is_main_process():\n",
                                     "                with open(log_path, \u0027w\u0027, newline=\u0027\u0027) as f:\n",
                                     "                    writer = csv.writer(f)\n",
                                     "                    writer.writerow([\n",
                                     "                        \u0027epoch\u0027, \u0027train_loss_g\u0027, \u0027val_loss_g\u0027, \u0027train_loss_d\u0027, \u0027val_loss_d\u0027,\n",
                                     "                        \u0027train_d_real_prob\u0027, \u0027val_d_real_prob\u0027, \u0027train_d_fake_prob\u0027, \u0027val_d_fake_prob\u0027,\n",
                                     "                        \u0027train_psnr\u0027, \u0027val_psnr\u0027, \u0027train_ssim\u0027, \u0027val_ssim\u0027, \u0027train_lpips\u0027, \u0027val_lpips\u0027,\n",
                                     "                    ])\n",
                                     "        if is_main_process():\n",
                                     "            print(f\"\\n{\u0027=\u0027*50}\")\n",
                                     "            print(f\"Starting from epoch {start_epoch}, best LPIPS: {best_lpips:.4f}\")\n",
                                     "            print(f\"Resume: {cfg[\u0027gan\u0027][\u0027resume\u0027]}, Load Disc: {cfg[\u0027gan\u0027][\u0027load_disc\u0027]}\")\n",
                                     "            print(f\"{\u0027=\u0027*50}\")\n",
                                     "\n",
                                     "        epochs = cfg[\u0027gan\u0027][\u0027epochs\u0027]\n",
                                     "        \n",
                                     "        for epoch in range(start_epoch, epochs + 1):\n",
                                     "            if hasattr(train_loader, \u0027sampler\u0027) and hasattr(train_loader.sampler, \u0027set_epoch\u0027):\n",
                                     "                train_loader.sampler.set_epoch(epoch)\n",
                                     "        \n",
                                     "            # ==================== Training ====================\n",
                                     "            train_pbar = tqdm(train_loader, desc=f\u0027Epoch {epoch}/{epochs} [Train]\u0027) if is_main_process() else train_loader\n",
                                     "            train_stats = train_gan_epoch(\n",
                                     "                generator, discriminator, train_pbar,\n",
                                     "                optimizer_g, optimizer_d, device,\n",
                                     "                pixel_criterion, perceptual_criterion, adversarial_criterion,\n",
                                     "                weights,\n",
                                     "                lpips_metric=lpips_metric,\n",
                                     "                g_steps=cfg[\u0027gan\u0027].get(\u0027g_steps\u0027, 1),\n",
                                     "                d_steps=cfg[\u0027gan\u0027].get(\u0027d_steps\u0027, 1),\n",
                                     "                r1_weight=cfg[\u0027gan\u0027].get(\u0027r1_weight\u0027, 0.0),\n",
                                     "                use_amp=use_amp,\n",
                                     "                scaler=scaler,\n",
                                     "            )\n",
                                     "        \n",
                                     "            # ==================== Validation ====================\n",
                                     "            val_pbar = tqdm(val_loader, desc=f\u0027Epoch {epoch}/{epochs} [Val]\u0027) if is_main_process() else val_loader\n",
                                     "            val_stats = val_gan_epoch(\n",
                                     "                generator, discriminator, val_pbar, device,\n",
                                     "                pixel_criterion, perceptual_criterion, adversarial_criterion,\n",
                                     "                weights,\n",
                                     "                lpips_metric=lpips_metric,\n",
                                     "                use_amp=use_amp\n",
                                     "            )\n",
                                     "        \n",
                                     "            # ==================== Step Schedulers ====================\n",
                                     "            scheduler_g.step()\n",
                                     "            scheduler_d.step()\n",
                                     "        \n",
                                     "            if is_main_process():\n",
                                     "                # ==================== Update History ====================\n",
                                     "                history[\u0027loss_g\u0027][\u0027train\u0027].append(train_stats[\u0027loss_g\u0027])\n",
                                     "                history[\u0027loss_g\u0027][\u0027val\u0027].append(val_stats[\u0027loss_g\u0027])\n",
                                     "                history[\u0027loss_d\u0027][\u0027train\u0027].append(train_stats[\u0027loss_d\u0027])\n",
                                     "                history[\u0027loss_d\u0027][\u0027val\u0027].append(val_stats[\u0027loss_d\u0027])\n",
                                     "                history[\u0027d_real_prob\u0027][\u0027train\u0027].append(train_stats[\u0027d_real_prob\u0027])\n",
                                     "                history[\u0027d_real_prob\u0027][\u0027val\u0027].append(val_stats[\u0027d_real_prob\u0027])\n",
                                     "                history[\u0027d_fake_prob\u0027][\u0027train\u0027].append(train_stats[\u0027d_fake_prob\u0027])\n",
                                     "                history[\u0027d_fake_prob\u0027][\u0027val\u0027].append(val_stats[\u0027d_fake_prob\u0027])\n",
                                     "                history[\u0027psnr\u0027][\u0027train\u0027].append(train_stats[\u0027psnr\u0027])\n",
                                     "                history[\u0027psnr\u0027][\u0027val\u0027].append(val_stats[\u0027psnr\u0027])\n",
                                     "                history[\u0027ssim\u0027][\u0027train\u0027].append(train_stats[\u0027ssim\u0027])\n",
                                     "                history[\u0027ssim\u0027][\u0027val\u0027].append(val_stats[\u0027ssim\u0027])\n",
                                     "                history[\u0027lpips\u0027][\u0027train\u0027].append(train_stats[\u0027lpips\u0027])\n",
                                     "                history[\u0027lpips\u0027][\u0027val\u0027].append(val_stats[\u0027lpips\u0027])\n",
                                     "        \n",
                                     "                # ==================== Append to Log ====================\n",
                                     "                with open(log_path, \u0027a\u0027, newline=\u0027\u0027) as f:\n",
                                     "                    writer = csv.writer(f)\n",
                                     "                    writer.writerow([\n",
                                     "                        epoch,\n",
                                     "                        train_stats[\u0027loss_g\u0027],\n",
                                     "                        val_stats[\u0027loss_g\u0027],\n",
                                     "                        train_stats[\u0027loss_d\u0027],\n",
                                     "                        val_stats[\u0027loss_d\u0027],\n",
                                     "                        train_stats[\u0027d_real_prob\u0027],\n",
                                     "                        val_stats[\u0027d_real_prob\u0027],\n",
                                     "                        train_stats[\u0027d_fake_prob\u0027],\n",
                                     "                        val_stats[\u0027d_fake_prob\u0027],\n",
                                     "                        train_stats[\u0027psnr\u0027],\n",
                                     "                        val_stats[\u0027psnr\u0027],\n",
                                     "                        train_stats[\u0027ssim\u0027],\n",
                                     "                        val_stats[\u0027ssim\u0027],\n",
                                     "                        train_stats[\u0027lpips\u0027],\n",
                                     "                        val_stats[\u0027lpips\u0027],\n",
                                     "                    ])\n",
                                     "        \n",
                                     "                # ==================== Save Checkpoint (every epoch) ====================\n",
                                     "                save_gan_checkpoint(\n",
                                     "                    generator=generator,\n",
                                     "                    discriminator=discriminator,\n",
                                     "                    optimizer_g=optimizer_g,\n",
                                     "                    optimizer_d=optimizer_d,\n",
                                     "                    scheduler_g=scheduler_g,\n",
                                     "                    scheduler_d=scheduler_d,\n",
                                     "                    epoch=epoch,\n",
                                     "                    best_lpips=best_lpips,\n",
                                     "                    path=cfg[\u0027gan\u0027][\u0027checkpoint_path\u0027]\n",
                                     "                )\n",
                                     "        \n",
                                     "                # Save individual weights (for compatibility)\n",
                                     "                gen_to_save = generator.module if hasattr(generator, \u0027module\u0027) else generator\n",
                                     "                disc_to_save = discriminator.module if hasattr(discriminator, \u0027module\u0027) else discriminator\n",
                                     "                torch.save(gen_to_save.state_dict(), \u0027weights/last_gan.pth\u0027)\n",
                                     "                torch.save(disc_to_save.state_dict(), \u0027weights/last_disc.pth\u0027)\n",
                                     "        \n",
                                     "                # ==================== Save Best Model ====================\n",
                                     "                if val_stats[\u0027lpips\u0027] \u003c best_lpips:\n",
                                     "                    best_lpips = val_stats[\u0027lpips\u0027]\n",
                                     "                    torch.save(gen_to_save.state_dict(), \u0027weights/best_gan.pth\u0027)\n",
                                     "                    torch.save(disc_to_save.state_dict(), \u0027weights/best_disc.pth\u0027)\n",
                                     "                    print(f\"[NEW BEST] LPIPS: {best_lpips:.4f}\")\n",
                                     "        \n",
                                     "                # ==================== Visualization ====================\n",
                                     "                clear_output(wait=True)\n",
                                     "        \n",
                                     "                rand_idx = random.randint(0, len(val_dataset) - 1)\n",
                                     "                lr_sample, hr_sample = val_dataset[rand_idx]\n",
                                     "                lr_in = lr_sample.unsqueeze(0).to(device)\n",
                                     "                with torch.no_grad():\n",
                                     "                    sr_sample = generator(lr_in).cpu()\n",
                                     "                show_lr_sr_hr(lr_sample, sr_sample, hr_sample)\n",
                                     "        \n",
                                     "                plot_curves(history)\n",
                                     "        \n",
                                     "                # Print info\n",
                                     "                print(f\"Epoch {epoch}/{epochs} | LR_G: {scheduler_g.get_last_lr()[0]:.6f} | LR_D: {scheduler_d.get_last_lr()[0]:.6f}\")\n",
                                     "                print(f\"Best LPIPS: {best_lpips:.4f}\")\n",
                                     "        \n",
                                     "        if is_main_process():\n",
                                     "            print(\"\\n\" + \"=\"*50)\n",
                                     "            print(\"GAN Training Completed!\")\n",
                                     "            print(f\"Best LPIPS: {best_lpips:.4f}\")\n",
                                     "            print(\"=\"*50)\n",
                                     "        \n",
                                     "        if is_ddp:\n",
                                     "            cleanup_distributed()\n",
                                     "    if __name__ == \"__main__\":\n",
                                     "        try:\n",
                                     "            mp.set_start_method(\u0027spawn\u0027, force=True)\n",
                                     "        except RuntimeError:\n",
                                     "            pass\n",
                                     "        world_size = torch.cuda.device_count()\n",
                                     "        mp.spawn(_ddp_worker, args=(world_size, cfg), nprocs=world_size, join=True)\n",
                                     "else:\n",
                                     "    epochs = cfg[\u0027gan\u0027][\u0027epochs\u0027]\n",
                                     "    \n",
                                     "    for epoch in range(start_epoch, epochs + 1):\n",
                                     "        if hasattr(train_loader, \u0027sampler\u0027) and hasattr(train_loader.sampler, \u0027set_epoch\u0027):\n",
                                     "            train_loader.sampler.set_epoch(epoch)\n",
                                     "    \n",
                                     "        # ==================== Training ====================\n",
                                     "        train_pbar = tqdm(train_loader, desc=f\u0027Epoch {epoch}/{epochs} [Train]\u0027) if is_main_process() else train_loader\n",
                                     "        train_stats = train_gan_epoch(\n",
                                     "            generator, discriminator, train_pbar,\n",
                                     "            optimizer_g, optimizer_d, device,\n",
                                     "            pixel_criterion, perceptual_criterion, adversarial_criterion,\n",
                                     "            weights,\n",
                                     "            lpips_metric=lpips_metric,\n",
                                     "            g_steps=cfg[\u0027gan\u0027].get(\u0027g_steps\u0027, 1),\n",
                                     "            d_steps=cfg[\u0027gan\u0027].get(\u0027d_steps\u0027, 1),\n",
                                     "            r1_weight=cfg[\u0027gan\u0027].get(\u0027r1_weight\u0027, 0.0),\n",
                                     "            use_amp=use_amp,\n",
                                     "            scaler=scaler,\n",
                                     "        )\n",
                                     "    \n",
                                     "        # ==================== Validation ====================\n",
                                     "        val_pbar = tqdm(val_loader, desc=f\u0027Epoch {epoch}/{epochs} [Val]\u0027) if is_main_process() else val_loader\n",
                                     "        val_stats = val_gan_epoch(\n",
                                     "            generator, discriminator, val_pbar, device,\n",
                                     "            pixel_criterion, perceptual_criterion, adversarial_criterion,\n",
                                     "            weights,\n",
                                     "            lpips_metric=lpips_metric,\n",
                                     "            use_amp=use_amp\n",
                                     "        )\n",
                                     "    \n",
                                     "        # ==================== Step Schedulers ====================\n",
                                     "        scheduler_g.step()\n",
                                     "        scheduler_d.step()\n",
                                     "    \n",
                                     "        if is_main_process():\n",
                                     "            # ==================== Update History ====================\n",
                                     "            history[\u0027loss_g\u0027][\u0027train\u0027].append(train_stats[\u0027loss_g\u0027])\n",
                                     "            history[\u0027loss_g\u0027][\u0027val\u0027].append(val_stats[\u0027loss_g\u0027])\n",
                                     "            history[\u0027loss_d\u0027][\u0027train\u0027].append(train_stats[\u0027loss_d\u0027])\n",
                                     "            history[\u0027loss_d\u0027][\u0027val\u0027].append(val_stats[\u0027loss_d\u0027])\n",
                                     "            history[\u0027d_real_prob\u0027][\u0027train\u0027].append(train_stats[\u0027d_real_prob\u0027])\n",
                                     "            history[\u0027d_real_prob\u0027][\u0027val\u0027].append(val_stats[\u0027d_real_prob\u0027])\n",
                                     "            history[\u0027d_fake_prob\u0027][\u0027train\u0027].append(train_stats[\u0027d_fake_prob\u0027])\n",
                                     "            history[\u0027d_fake_prob\u0027][\u0027val\u0027].append(val_stats[\u0027d_fake_prob\u0027])\n",
                                     "            history[\u0027psnr\u0027][\u0027train\u0027].append(train_stats[\u0027psnr\u0027])\n",
                                     "            history[\u0027psnr\u0027][\u0027val\u0027].append(val_stats[\u0027psnr\u0027])\n",
                                     "            history[\u0027ssim\u0027][\u0027train\u0027].append(train_stats[\u0027ssim\u0027])\n",
                                     "            history[\u0027ssim\u0027][\u0027val\u0027].append(val_stats[\u0027ssim\u0027])\n",
                                     "            history[\u0027lpips\u0027][\u0027train\u0027].append(train_stats[\u0027lpips\u0027])\n",
                                     "            history[\u0027lpips\u0027][\u0027val\u0027].append(val_stats[\u0027lpips\u0027])\n",
                                     "    \n",
                                     "            # ==================== Append to Log ====================\n",
                                     "            with open(log_path, \u0027a\u0027, newline=\u0027\u0027) as f:\n",
                                     "                writer = csv.writer(f)\n",
                                     "                writer.writerow([\n",
                                     "                    epoch,\n",
                                     "                    train_stats[\u0027loss_g\u0027],\n",
                                     "                    val_stats[\u0027loss_g\u0027],\n",
                                     "                    train_stats[\u0027loss_d\u0027],\n",
                                     "                    val_stats[\u0027loss_d\u0027],\n",
                                     "                    train_stats[\u0027d_real_prob\u0027],\n",
                                     "                    val_stats[\u0027d_real_prob\u0027],\n",
                                     "                    train_stats[\u0027d_fake_prob\u0027],\n",
                                     "                    val_stats[\u0027d_fake_prob\u0027],\n",
                                     "                    train_stats[\u0027psnr\u0027],\n",
                                     "                    val_stats[\u0027psnr\u0027],\n",
                                     "                    train_stats[\u0027ssim\u0027],\n",
                                     "                    val_stats[\u0027ssim\u0027],\n",
                                     "                    train_stats[\u0027lpips\u0027],\n",
                                     "                    val_stats[\u0027lpips\u0027],\n",
                                     "                ])\n",
                                     "    \n",
                                     "            # ==================== Save Checkpoint (every epoch) ====================\n",
                                     "            save_gan_checkpoint(\n",
                                     "                generator=generator,\n",
                                     "                discriminator=discriminator,\n",
                                     "                optimizer_g=optimizer_g,\n",
                                     "                optimizer_d=optimizer_d,\n",
                                     "                scheduler_g=scheduler_g,\n",
                                     "                scheduler_d=scheduler_d,\n",
                                     "                epoch=epoch,\n",
                                     "                best_lpips=best_lpips,\n",
                                     "                path=cfg[\u0027gan\u0027][\u0027checkpoint_path\u0027]\n",
                                     "            )\n",
                                     "    \n",
                                     "            # Save individual weights (for compatibility)\n",
                                     "            gen_to_save = generator.module if hasattr(generator, \u0027module\u0027) else generator\n",
                                     "            disc_to_save = discriminator.module if hasattr(discriminator, \u0027module\u0027) else discriminator\n",
                                     "            torch.save(gen_to_save.state_dict(), \u0027weights/last_gan.pth\u0027)\n",
                                     "            torch.save(disc_to_save.state_dict(), \u0027weights/last_disc.pth\u0027)\n",
                                     "    \n",
                                     "            # ==================== Save Best Model ====================\n",
                                     "            if val_stats[\u0027lpips\u0027] \u003c best_lpips:\n",
                                     "                best_lpips = val_stats[\u0027lpips\u0027]\n",
                                     "                torch.save(gen_to_save.state_dict(), \u0027weights/best_gan.pth\u0027)\n",
                                     "                torch.save(disc_to_save.state_dict(), \u0027weights/best_disc.pth\u0027)\n",
                                     "                print(f\"[NEW BEST] LPIPS: {best_lpips:.4f}\")\n",
                                     "    \n",
                                     "            # ==================== Visualization ====================\n",
                                     "            clear_output(wait=True)\n",
                                     "    \n",
                                     "            rand_idx = random.randint(0, len(val_dataset) - 1)\n",
                                     "            lr_sample, hr_sample = val_dataset[rand_idx]\n",
                                     "            lr_in = lr_sample.unsqueeze(0).to(device)\n",
                                     "            with torch.no_grad():\n",
                                     "                sr_sample = generator(lr_in).cpu()\n",
                                     "            show_lr_sr_hr(lr_sample, sr_sample, hr_sample)\n",
                                     "    \n",
                                     "            plot_curves(history)\n",
                                     "    \n",
                                     "            # Print info\n",
                                     "            print(f\"Epoch {epoch}/{epochs} | LR_G: {scheduler_g.get_last_lr()[0]:.6f} | LR_D: {scheduler_d.get_last_lr()[0]:.6f}\")\n",
                                     "            print(f\"Best LPIPS: {best_lpips:.4f}\")\n",
                                     "    \n",
                                     "    if is_main_process():\n",
                                     "        print(\"\\n\" + \"=\"*50)\n",
                                     "        print(\"GAN Training Completed!\")\n",
                                     "        print(f\"Best LPIPS: {best_lpips:.4f}\")\n",
                                     "        print(\"=\"*50)\n",
                                     "    \n",
                                     "    if is_ddp:\n",
                                     "        cleanup_distributed()\n"
                                 ]
                  }
              ],
    "metadata":  {
                     "kernelspec":  {
                                        "display_name":  "(venv) Python 3.13.0 (3.13.0)",
                                        "language":  "python",
                                        "name":  "python3"
                                    },
                     "language_info":  {
                                           "codemirror_mode":  {
                                                                   "name":  "ipython",
                                                                   "version":  3
                                                               },
                                           "file_extension":  ".py",
                                           "mimetype":  "text/x-python",
                                           "name":  "python",
                                           "nbconvert_exporter":  "python",
                                           "pygments_lexer":  "ipython3",
                                           "version":  "3.13.0"
                                       }
                 },
    "nbformat":  4,
    "nbformat_minor":  5
}
