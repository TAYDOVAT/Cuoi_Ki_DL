{
    "cells":  [
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "setup",
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "# !rm -rf /kaggle/working/*\n",
                                     "# %cd /kaggle/working\n",
                                     "# !git clone --branch Distributed-Data-Parallel https://github.com/TAYDOVAT/Cuoi_Ki_DL.git\n",
                                     "# !pip install lpips\n",
                                     "# %cd /kaggle/working/Cuoi_Ki_DL\n",
                                     "\n",
                                     "# !rm -r /kaggle/working/Cuoi_Ki_DL/test\n",
                                     "# !rm -r /kaggle/working/Cuoi_Ki_DL/train\n",
                                     "# !rm -r /kaggle/working/Cuoi_Ki_DL/val\n"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "id":  "title",
                      "metadata":  {

                                   },
                      "source":  [
                                     "# Train SRGAN x4 (DDP torchrun)"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "imports",
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "import os\n",
                                     "import json\n",
                                     "from pathlib import Path\n",
                                     "import torch\n"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "config",
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "# Config override here\n",
                                     "cfg = {\n",
                                     "    \u0027scale\u0027: 4,\n",
                                     "    \u0027hr_crop\u0027: 96,\n",
                                     "    \u0027gan\u0027: {\n",
                                     "        \u0027batch_size\u0027: 32,\n",
                                     "        \u0027num_workers\u0027: 4,\n",
                                     "        \u0027epochs\u0027: 300,\n",
                                     "        \u0027lr_g\u0027: 1e-5,           # Learning rate cho Generator\n",
                                     "        \u0027lr_d\u0027: 1e-5,           # Learning rate cho Discriminator\n",
                                     "        \u0027adv_weight\u0027: 1e-2,     # Adversarial loss weight\n",
                                     "        \u0027perc_weight\u0027: 1,       # Perceptual loss weight\n",
                                     "        \u0027pixel_weight\u0027: 0,      # Pixel loss weight\n",
                                     "        \u0027r1_weight\u0027: 10.0,      # R1 gradient penalty\n",
                                     "        \u0027use_amp\u0027: True,        # Automatic Mixed Precision\n",
                                     "        \u0027d_steps\u0027: 1,           # So buoc train D moi iteration\n",
                                     "        \u0027g_steps\u0027: 2,           # So buoc train G moi iteration\n",
                                     "        # ========== RESUME CONFIG ==========\n",
                                     "        \u0027resume\u0027: False,        # True: resume training, False: train tu dau\n",
                                     "        \u0027load_disc\u0027: False,     # True: load ca Discriminator, False: chi load Generator\n",
                                     "        \u0027checkpoint_path\u0027: \u0027weights/gan_checkpoint.pth\u0027,  # Path to checkpoint\n",
                                     "    },\n",
                                     "    \u0027paths\u0027: {\n",
                                     "        \u0027train_lr\u0027: \u0027train/train_lr\u0027,\n",
                                     "        \u0027train_hr\u0027: \u0027train/train_hr\u0027,\n",
                                     "        \u0027val_lr\u0027: \u0027val/val_lr\u0027,\n",
                                     "        \u0027val_hr\u0027: \u0027val/val_hr\u0027,\n",
                                     "        \u0027test_lr\u0027: \u0027test/test_lr\u0027,\n",
                                     "        \u0027test_hr\u0027: \u0027test/test_hr\u0027,\n",
                                     "    },\n",
                                     "}\n",
                                     "\n",
                                     "data_root = os.environ.get(\u0027DATA_ROOT\u0027)\n",
                                     "kaggle_root = Path(\u0027/kaggle/input/anh-ve-tinh-2/Anh_ve_tinh_2\u0027)\n",
                                     "base_dir = None\n",
                                     "cwd = Path.cwd().resolve()\n",
                                     "\n",
                                     "candidate = None\n",
                                     "if data_root:\n",
                                     "    candidate = Path(data_root).expanduser().resolve()\n",
                                     "elif kaggle_root.is_dir():\n",
                                     "    candidate = kaggle_root\n",
                                     "\n",
                                     "if candidate is not None and (candidate / \u0027train\u0027 / \u0027train_lr\u0027).is_dir():\n",
                                     "    base_dir = candidate\n",
                                     "else:\n",
                                     "    train_lr_path = Path(cfg[\u0027paths\u0027][\u0027train_lr\u0027])\n",
                                     "    if train_lr_path.is_absolute():\n",
                                     "        if train_lr_path.is_dir():\n",
                                     "            base_dir = train_lr_path.parents[1]\n",
                                     "    else:\n",
                                     "        candidate = (cwd / train_lr_path).resolve()\n",
                                     "        if candidate.is_dir():\n",
                                     "            base_dir = candidate.parents[1]\n",
                                     "\n",
                                     "if base_dir is None:\n",
                                     "    for parent in [cwd] + list(cwd.parents):\n",
                                     "        if (parent / \u0027train\u0027 / \u0027train_lr\u0027).is_dir():\n",
                                     "            base_dir = parent\n",
                                     "            break\n",
                                     "\n",
                                     "if base_dir is None:\n",
                                     "    raise FileNotFoundError(\n",
                                     "        f\"Cannot find dataset root. Set DATA_ROOT or update cfg[\u0027paths\u0027] (cwd: {cwd})\"\n",
                                     "    )\n",
                                     "\n",
                                     "cfg[\u0027paths\u0027][\u0027train_lr\u0027] = str(base_dir / \u0027train\u0027 / \u0027train_lr\u0027)\n",
                                     "cfg[\u0027paths\u0027][\u0027train_hr\u0027] = str(base_dir / \u0027train\u0027 / \u0027train_hr\u0027)\n",
                                     "cfg[\u0027paths\u0027][\u0027val_lr\u0027] = str(base_dir / \u0027val\u0027 / \u0027val_lr\u0027)\n",
                                     "cfg[\u0027paths\u0027][\u0027val_hr\u0027] = str(base_dir / \u0027val\u0027 / \u0027val_hr\u0027)\n",
                                     "cfg[\u0027paths\u0027][\u0027test_lr\u0027] = str(base_dir / \u0027test\u0027 / \u0027test_lr\u0027)\n",
                                     "cfg[\u0027paths\u0027][\u0027test_hr\u0027] = str(base_dir / \u0027test\u0027 / \u0027test_hr\u0027)\n",
                                     "\n",
                                     "config_path = \u0027configs_gan.json\u0027\n",
                                     "with open(config_path, \u0027w\u0027) as f:\n",
                                     "    json.dump(cfg, f, indent=2)\n",
                                     "print(f\"Wrote {config_path}\")\n"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "id":  "run_md",
                      "metadata":  {

                                   },
                      "source":  [
                                     "## Run torchrun DDP"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "run",
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "import subprocess\n",
                                     "import sys\n",
                                     "\n",
                                     "nproc = torch.cuda.device_count()\n",
                                     "if nproc \u003c 2:\n",
                                     "    raise RuntimeError(\u0027torchrun DDP requires \u003e=2 GPUs\u0027)\n",
                                     "\n",
                                     "cmd = [\n",
                                     "    \u0027torchrun\u0027, \u0027--standalone\u0027, f\u0027--nproc_per_node={nproc}\u0027,\n",
                                     "    \u0027train_gan_ddp.py\u0027, \u0027--config\u0027, config_path\n",
                                     "]\n",
                                     "print(\u0027Launching:\u0027, \u0027 \u0027.join(cmd))\n",
                                     "proc = subprocess.Popen(cmd)\n",
                                     "print(f\u0027Background PID: {proc.pid}\u0027)\n"
                                 ]
                  },
                  {
                      "cell_type":  "markdown",
                      "id":  "monitor_md",
                      "metadata":  {

                                   },
                      "source":  [
                                     "## Monitor log and plots (epoch-level)"
                                 ]
                  },
                  {
                      "cell_type":  "code",
                      "execution_count":  null,
                      "id":  "monitor",
                      "metadata":  {

                                   },
                      "outputs":  [

                                  ],
                      "source":  [
                                     "import time\n",
                                     "import csv\n",
                                     "import math\n",
                                     "import matplotlib.pyplot as plt\n",
                                     "from IPython.display import clear_output\n",
                                     "\n",
                                     "LOG_PATH = \u0027logs/gan_log.csv\u0027\n",
                                     "TOTAL_EPOCHS = cfg[\u0027gan\u0027][\u0027epochs\u0027]\n",
                                     "REFRESH_SEC = 5\n",
                                     "\n",
                                     "def read_log(path):\n",
                                     "    if not os.path.exists(path):\n",
                                     "        return []\n",
                                     "    try:\n",
                                     "        with open(path, \u0027r\u0027, newline=\u0027\u0027) as f:\n",
                                     "            reader = csv.DictReader(f)\n",
                                     "            return list(reader)\n",
                                     "    except Exception:\n",
                                     "        return []\n",
                                     "\n",
                                     "def render_bar(cur, total, width=30):\n",
                                     "    if total \u003c= 0:\n",
                                     "        return \u0027[?]\u0027\n",
                                     "    cur = min(cur, total)\n",
                                     "    filled = int(width * cur / total)\n",
                                     "    return f\"[{\u0027#\u0027 * filled}{\u0027.\u0027 * (width - filled)}] {cur}/{total}\"\n",
                                     "\n",
                                     "while True:\n",
                                     "    rows = read_log(LOG_PATH)\n",
                                     "    clear_output(wait=True)\n",
                                     "    if not rows:\n",
                                     "        print(\u0027Chua co log. Doi...\u0027)\n",
                                     "        time.sleep(REFRESH_SEC)\n",
                                     "        continue\n",
                                     "\n",
                                     "    last = rows[-1]\n",
                                     "    epoch = int(last[\u0027epoch\u0027])\n",
                                     "    print(\u0027Progress:\u0027, render_bar(epoch, TOTAL_EPOCHS))\n",
                                     "    print(f\"Epoch {epoch}/{TOTAL_EPOCHS}\")\n",
                                     "    print(\n",
                                     "        f\"Train G: {float(last[\u0027train_loss_g\u0027]):.4f} | \"\n",
                                     "        f\"Val G: {float(last[\u0027val_loss_g\u0027]):.4f} | \"\n",
                                     "        f\"LPIPS Val: {float(last[\u0027val_lpips\u0027]):.4f}\"\n",
                                     "    )\n",
                                     "\n",
                                     "    epochs = [int(r[\u0027epoch\u0027]) for r in rows]\n",
                                     "\n",
                                     "    train_loss_g = [float(r[\u0027train_loss_g\u0027]) for r in rows]\n",
                                     "    val_loss_g = [float(r[\u0027val_loss_g\u0027]) for r in rows]\n",
                                     "    train_loss_d = [float(r[\u0027train_loss_d\u0027]) for r in rows]\n",
                                     "    val_loss_d = [float(r[\u0027val_loss_d\u0027]) for r in rows]\n",
                                     "\n",
                                     "    train_psnr = [float(r[\u0027train_psnr\u0027]) for r in rows]\n",
                                     "    val_psnr = [float(r[\u0027val_psnr\u0027]) for r in rows]\n",
                                     "    train_ssim = [float(r[\u0027train_ssim\u0027]) for r in rows]\n",
                                     "    val_ssim = [float(r[\u0027val_ssim\u0027]) for r in rows]\n",
                                     "    train_lpips = [float(r[\u0027train_lpips\u0027]) for r in rows]\n",
                                     "    val_lpips = [float(r[\u0027val_lpips\u0027]) for r in rows]\n",
                                     "\n",
                                     "    train_d_real = [float(r[\u0027train_d_real_prob\u0027]) for r in rows]\n",
                                     "    val_d_real = [float(r[\u0027val_d_real_prob\u0027]) for r in rows]\n",
                                     "    train_d_fake = [float(r[\u0027train_d_fake_prob\u0027]) for r in rows]\n",
                                     "    val_d_fake = [float(r[\u0027val_d_fake_prob\u0027]) for r in rows]\n",
                                     "\n",
                                     "    plots = [\n",
                                     "        (\u0027loss_g\u0027, train_loss_g, val_loss_g),\n",
                                     "        (\u0027loss_d\u0027, train_loss_d, val_loss_d),\n",
                                     "        (\u0027psnr\u0027, train_psnr, val_psnr),\n",
                                     "        (\u0027ssim\u0027, train_ssim, val_ssim),\n",
                                     "        (\u0027lpips\u0027, train_lpips, val_lpips),\n",
                                     "        (\u0027d_real_prob\u0027, train_d_real, val_d_real),\n",
                                     "        (\u0027d_fake_prob\u0027, train_d_fake, val_d_fake),\n",
                                     "    ]\n",
                                     "\n",
                                     "    ncols = 3\n",
                                     "    nrows = math.ceil(len(plots) / ncols)\n",
                                     "    fig, axes = plt.subplots(nrows, ncols, figsize=(15, 4 * nrows))\n",
                                     "    axes = axes.flatten() if hasattr(axes, \u0027flatten\u0027) else [axes]\n",
                                     "\n",
                                     "    for i, (title, train_vals, val_vals) in enumerate(plots):\n",
                                     "        ax = axes[i]\n",
                                     "        ax.plot(epochs, train_vals, label=\u0027train\u0027)\n",
                                     "        ax.plot(epochs, val_vals, label=\u0027val\u0027)\n",
                                     "        ax.set_title(title)\n",
                                     "        ax.grid(True, alpha=0.3)\n",
                                     "        ax.legend()\n",
                                     "\n",
                                     "    # Hide any unused subplots\n",
                                     "    for j in range(len(plots), len(axes)):\n",
                                     "        axes[j].axis(\u0027off\u0027)\n",
                                     "\n",
                                     "    plt.tight_layout()\n",
                                     "    plt.show()\n",
                                     "\n",
                                     "    time.sleep(REFRESH_SEC)\n"
                                 ]
                  }
              ],
    "metadata":  {
                     "kernelspec":  {
                                        "display_name":  "(venv) Python 3.13.0 (3.13.0)",
                                        "language":  "python",
                                        "name":  "python3"
                                    },
                     "language_info":  {
                                           "codemirror_mode":  {
                                                                   "name":  "ipython",
                                                                   "version":  3
                                                               },
                                           "file_extension":  ".py",
                                           "mimetype":  "text/x-python",
                                           "name":  "python",
                                           "nbconvert_exporter":  "python",
                                           "pygments_lexer":  "ipython3",
                                           "version":  "3.13.0"
                                       }
                 },
    "nbformat":  4,
    "nbformat_minor":  5
}
